{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './hour.csv'\n",
    "\n",
    "rides = pd.read_csv(data_path)\n",
    "\n",
    "rides.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:24*10].plot(x='dteday', y='cnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_fields = ['season', 'weathersit', 'mnth', 'hr', 'weekday']\n",
    "for each in dummy_fields:\n",
    "    dummies = pd.get_dummies(rides[each], prefix=each, drop_first=False)\n",
    "    rides = pd.concat([rides, dummies], axis=1)\n",
    "\n",
    "fields_to_drop = ['instant', 'dteday', 'season', 'weathersit', \n",
    "                  'weekday', 'atemp', 'mnth', 'workingday', 'hr']\n",
    "data = rides.drop(fields_to_drop, axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_features = ['casual', 'registered', 'cnt', 'temp', 'hum', 'windspeed']\n",
    "# Store scalings in a dictionary so we can convert back later\n",
    "scaled_features = {}\n",
    "for each in quant_features:\n",
    "    mean, std = data[each].mean(), data[each].std()\n",
    "    scaled_features[each] = [mean, std]\n",
    "    data.loc[:, each] = (data[each] - mean)/std\n",
    "    \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = data[-21*24:]\n",
    "data = data[:-21*24]\n",
    "\n",
    "target_fields = ['cnt', 'casual', 'registered']\n",
    "features, targets = data.drop(target_fields, axis=1), data[target_fields]\n",
    "test_features, test_targets = test_data.drop(target_fields, axis=1), test_data[target_fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_targets = features[:-60*24], targets[:-60*24]\n",
    "val_features, val_targets = features[-60*24:], targets[-60*24:]\n",
    "\n",
    "print(val_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "save_file = 'model.ckpt'\n",
    "input_nodes = train_features.shape[1]\n",
    "hidden_nodes = 15\n",
    "output_nodes = 1\n",
    "batch_size = 128\n",
    "epochs = 1500\n",
    "learning_rate = 0.01\n",
    "display_rate = 10\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape = (None, input_nodes), name = 'x')\n",
    "y = tf.placeholder(tf.float32, shape = (None, 1), name = 'y')\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal(shape = (input_nodes, hidden_nodes), stddev = 0.1), name = 'W1')\n",
    "W2 = tf.Variable(tf.truncated_normal(shape = (hidden_nodes, output_nodes), stddev = 0.1), name = 'W2')\n",
    "\n",
    "h1 = tf.nn.sigmoid(tf.matmul(x, W1))\n",
    "output = tf.matmul(h1, W2)\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(tf.square(tf.subtract(output, y))), name = 'cost')\n",
    "tf.summary.scalar('cost', cost)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "writer = tf.summary.FileWriter(logdir = './graphs')\n",
    "\n",
    "summary = tf.summary.merge_all()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    writer.add_graph(sess.graph)\n",
    "    for i in range(epochs):\n",
    "        batch = np.random.choice(train_features.index, size=128)\n",
    "        X, Y = train_features.ix[batch].values, train_targets.ix[batch]['cnt']\n",
    "        Y = np.reshape(Y, (Y.shape[0], 1))\n",
    "        feed_dict = {x: X, y: Y}\n",
    "        sess.run(train_op, feed_dict = feed_dict)\n",
    "        if i % 10 == 0:\n",
    "            c_cost, summary_result = sess.run([cost, summary] , feed_dict = feed_dict)\n",
    "            writer.add_summary(summary_result, i)\n",
    "            print(\"The current cost is {0}\".format(c_cost))\n",
    "\n",
    "    val_targets_reshaped = np.reshape(val_targets['cnt'], (val_targets['cnt'].shape[0], 1))\n",
    "    validation_cost = sess.run(cost, feed_dict = {x: val_features, y: val_targets_reshaped})\n",
    "    print(\"The validation cost is {0}\".format(validation_cost))\n",
    "    saver.save(sess, save_file)\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "save_file = 'model.ckpt'\n",
    "input_nodes = train_features.shape[1]\n",
    "hidden_nodes = 15\n",
    "output_nodes = 1\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape = (None, input_nodes), name = 'x')\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal(shape = (input_nodes, hidden_nodes), stddev = 0.1), name = 'W1')\n",
    "W2 = tf.Variable(tf.truncated_normal(shape = (hidden_nodes, output_nodes), stddev = 0.1), name = 'W2')\n",
    "\n",
    "h1 = tf.nn.sigmoid(tf.matmul(x, W1))\n",
    "output = tf.matmul(h1, W2)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, save_file)\n",
    "    predictions = sess.run(output, feed_dict = {x: test_features})\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "mean, std = scaled_features['cnt']\n",
    "predictions = predictions * std + mean\n",
    "ax.plot(predictions, label='Prediction')\n",
    "ax.plot((test_targets['cnt']*std + mean).values, label='Data')\n",
    "ax.set_xlim(right=len(predictions))\n",
    "ax.legend()\n",
    "\n",
    "dates = pd.to_datetime(rides.ix[test_data.index]['dteday'])\n",
    "dates = dates.apply(lambda d: d.strftime('%b %d'))\n",
    "ax.set_xticks(np.arange(len(dates))[12::24])\n",
    "_ = ax.set_xticklabels(dates[12::24], rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
